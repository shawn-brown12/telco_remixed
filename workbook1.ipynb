{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e290b-e45f-4661-8cdf-565448f744b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the imports needed to use the following functions\n",
    "import os\n",
    "import pandas as pd\n",
    "#this will connect to Codeup mysql server if this function is within the env.py file in directory\n",
    "from env import host, username, password\n",
    "\n",
    "\n",
    "def get_connection(db, user=username, host=host, password=password):\n",
    "    \n",
    "    '''\n",
    "    This function is to connect to the Codeup MySQL server, and by itself won't do anything. It works in conjunction with \n",
    "    the  other functions within this .py file.\n",
    "    '''\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "\n",
    "def get_titanic_data():\n",
    "\n",
    "    '''\n",
    "   This function will check locally if there's a titanic.csv file in the local directory, and if not, working with the \n",
    "    get_connection function, will pull the titanic dataset from the Codeup MySQL server. After that, it will also save a copy of \n",
    "    the csv locally if there wasn't one, so it doesn't have to run the query each time.\n",
    "    '''\n",
    "    if os.path.isfile('titanic.csv'):\n",
    "        return pd.read_csv('titanic.csv')\n",
    "    else:\n",
    "        url = get_connection('titanic_db')\n",
    "        query = '''\n",
    "                SELECT *\n",
    "                FROM passengers\n",
    "                '''\n",
    "        titanic = pd.read_sql(query, url)\n",
    "        titanic.to_csv('titanic.csv')\n",
    "        return titanic\n",
    "\n",
    "def get_iris_data():\n",
    "    '''\n",
    "    This function will check locally if there's a iris.csv file in the local directory, and if not, working with the \n",
    "    get_connection function, will pull the iris dataset from the Codeup MySQL server. After that, it will also save a copy of \n",
    "    the csv locally if there wasn't one, so it doesn't have to run the query each time.\n",
    "    '''\n",
    "    if os.path.isfile('iris.csv'):\n",
    "        return pd.read_csv('iris.csv')\n",
    "    else:\n",
    "        url = get_connection('iris_db')\n",
    "        query = '''\n",
    "                SELECT *\n",
    "                FROM species\n",
    "                JOIN measurements USING(species_id);\n",
    "                '''\n",
    "        iris = pd.read_sql(query, url)\n",
    "        iris.to_csv('iris.csv')\n",
    "        return iris\n",
    "\n",
    "def get_telco_data():\n",
    "    '''\n",
    "    This function will check locally if there's a telco.csv file in the local directory, and if not, working with the \n",
    "    get_connection function, will pull the telco dataset from the Codeup MySQL server. After that, it will also save a copy of \n",
    "    the csv locally if there wasn't one, so it doesn't have to run the query each time.\n",
    "    '''\n",
    "    if os.path.isfile('telco.csv'):\n",
    "        return pd.read_csv('telco.csv')\n",
    "    else:\n",
    "        url = get_connection('telco_churn')\n",
    "        query = '''\n",
    "                SELECT *\n",
    "                FROM customers\n",
    "                JOIN contract_types USING(contract_type_id)\n",
    "                JOIN internet_service_types USING (internet_service_type_id)\n",
    "                JOIN payment_types types USING(payment_type_id);\n",
    "                '''\n",
    "        telco = pd.read_sql(query, url)\n",
    "        telco.to_csv('telco.csv')\n",
    "        return telco\n",
    "    \n",
    "#this will return the data from specifically the attendance table in the tidy data dataset\n",
    "def get_attendance_data():\n",
    "    '''\n",
    "    This function will check locally if there's a attendance.csv file in the local directory, and if not, working with the \n",
    "    get_connection function, will pull the attendance dataset from the tidy_data database in the Codeup MySQL server. After that,\n",
    "    it will also save a copy of the csv locally if there wasn't one, so it doesn't have to run the query each time.\n",
    "    '''\n",
    "    if os.path.isfile('attendance.csv'):\n",
    "        return pd.read_csv('attendance.csv')\n",
    "    else:\n",
    "        url = get_connection('tidy_data')\n",
    "        query = '''\n",
    "                SELECT *\n",
    "                FROM attendance\n",
    "                '''\n",
    "        attendance = pd.read_sql(query, url)\n",
    "        attendance.to_csv('attendance.csv')\n",
    "        attendance.drop(columns='Unnamed: 0.1')\n",
    "        return attendance\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f8fd7-ec1c-47f2-97d8-5a73de61f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the imports needed to run the functions within this file\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#this will connect to Codeup mysql server if this function is within the env.py file in directory\n",
    "from env import host, username, password\n",
    "\n",
    "def split_train_test(df, col, seed=42):\n",
    "    '''\n",
    "    This function will split a dataset into train, validate, and test variables to model with. Make sure to assign to three \n",
    "    variables when running.\n",
    "    '''\n",
    "    seed = 123\n",
    "    train, val_test = train_test_split(df, train_size=.6, random_state=123, stratify=df[col])\n",
    "    validate, test = train_test_split(val_test, train_size=.6, random_state=123, stratify=val_test[col])\n",
    "    \n",
    "    return train, validate, test\n",
    "\n",
    "def prep_iris(df):\n",
    "    '''\n",
    "    This function is used to prepare the iris dataset. It will drop several unneeded columns, rename the 'species_name' to just \n",
    "    'species', and create and concatenate dummies.\n",
    "    '''\n",
    "    to_drop = ['species_id', 'measurement_id', 'Unnamed: 0']\n",
    "    df.drop(columns=to_drop, inplace=True)\n",
    "    df = df.rename(columns={'species_name':'species'})\n",
    "    \n",
    "    dummies = pd.get_dummies(df[['species']], drop_first=True)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prep_titanic(df):\n",
    "    '''\n",
    "    This is used to prepare the titanic dataset to work with. It will drop unneeded columns, as well as create and then \n",
    "    concatenate dummies onto the DataFrame\n",
    "    '''\n",
    "    to_drop = ['Unnamed: 0', 'class', 'embarked', 'passenger_id', 'deck', 'age']\n",
    "    df.drop(columns=to_drop, inplace=True)\n",
    "    \n",
    "    dummies = pd.get_dummies(df[['sex', 'embark_town']], drop_first=True)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "    df = df.drop(columns=['sex', 'embark_town'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prep_telco(df):\n",
    "    '''\n",
    "    This function will prepare the telco_churn dataset for further use. It will convert the total_charges column into the float \n",
    "    data type, create various dummies and concatenate those dummies, and then drop unneeded columns.\n",
    "    '''\n",
    "    telco['total_charges'] = telco['total_charges'].replace(' ', '0')\n",
    "    telco['total_charges'] = telco['total_charges'].astype(float)\n",
    "    \n",
    "    to_dummy = ['gender', 'partner', 'dependents', 'phone_service', 'multiple_lines', 'online_security', \n",
    "                'online_backup', 'device_protection', 'tech_support', 'streaming_tv', 'streaming_movies', \n",
    "                'paperless_billing', 'contract_type', 'internet_service_type', \n",
    "                'payment_type']\n",
    "    dummies = pd.get_dummies(df[to_dummy], drop_first=True)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    \n",
    "    drop = ['gender', 'partner', 'dependents', 'phone_service', 'multiple_lines', 'online_security', \n",
    "                'online_backup', 'device_protection', 'tech_support', 'streaming_tv', 'streaming_movies', \n",
    "                'paperless_billing', 'contract_type', 'internet_service_type', \n",
    "                'payment_type', 'Unnamed: 0', 'payment_type_id', 'internet_service_type_id', 'contract_type_id', 'customer_id']\n",
    "    df.drop(columns=drop, inplace=True)\n",
    "                 \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
